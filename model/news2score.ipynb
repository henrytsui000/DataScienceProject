{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PATH = 'bert-base-uncased'\n",
    "MAX_LEN = 64\n",
    "DEVICE = \"cuda\"\n",
    "BZ = 128\n",
    "WD = 1e-9\n",
    "LR = 1e-5\n",
    "EPS = 50\n",
    "TMAX = 15\n",
    "COMMENT = f\"lr{LR}-B{BZ}-EPS{EPS}-TOU{TMAX}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.read_csv(\"../data/tmp.csv\")\n",
    "# , index_col=0\n",
    "merge = shuffle(merge)\n",
    "ds_size = merge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      symbol   datentime                                            content  \\\n",
      "16192   AMZN  2021-11-03  Lannebo Fonder AB Buys Upland Software Inc, Se...   \n",
      "4971    AMZN  2022-07-24  ARGI Investment Services LLC Purchases 107 Sha...   \n",
      "24899   AMZN  2021-03-16  UAVS ALERT: Kessler Topaz Meltzer & Check, LLP...   \n",
      "13349   NFLX  2022-01-11  Blackstone-backed Candle Media acquires Farawa...   \n",
      "14239   AMZN  2021-12-18  Amazon collaborated with China propaganda arm:...   \n",
      "...      ...         ...                                                ...   \n",
      "14142   AAPL  2021-12-20                         SPY Stock: The S&P 500 ETF   \n",
      "17680   AAPL  2021-10-07  Discovery : BOOM TIME FOR MOONSHINE! ALL-NEW S...   \n",
      "8147    NFLX  2022-05-14  Top 5 1st Quarter Trades of Winslow Capital Ma...   \n",
      "10347   NFLX  2022-03-15  Euronet Stock Offers Undervalued Leverage To N...   \n",
      "833     META  2022-10-19  Silvergate Reports a Fall After Its Delayed St...   \n",
      "\n",
      "            match  sentiment      score  preprice  nowprice  nextprice  \n",
      "16192   22.828592   0.526700  12.023819  165.6375  169.2000   173.8500  \n",
      "4971   123.195110   0.313664  38.641871  122.4200  121.1400   114.8100  \n",
      "24899   14.734700  -0.410700  -6.051541  154.0840  154.5930   156.7865  \n",
      "13349   12.887804   0.421500   5.432209  539.8500  540.8400   537.2200  \n",
      "14239   10.484009   0.096033   1.006811  170.0175  167.0790   170.4170  \n",
      "...           ...        ...        ...       ...       ...        ...  \n",
      "14142   21.995184   0.721700  15.873924  170.1525  168.7705   171.9918  \n",
      "17680    6.221320   0.177900   1.106773  140.9749  142.2556   141.8684  \n",
      "8147    13.377523   0.671900   8.988358  187.6400  186.5100   190.5600  \n",
      "10347   10.765780   0.361200   3.888600  331.0100  343.7500   357.5300  \n",
      "833     14.534845  -0.624900  -9.082825  132.8000  133.2300   131.5300  \n",
      "\n",
      "[25278 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock(Dataset):\n",
    "    def __init__(self, df) -> None:\n",
    "        self.df = df\n",
    "        self.score = [float(score) for score in df[\"score\"]]\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "        self.content = [self.tokenizer(text,padding='max_length', \n",
    "                       max_length = MAX_LEN, \n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\") for text in df[\"content\"]]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.score[idx], self.content[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 293kB/s]  \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 7.25kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 214kB/s]\n",
      "/Users/lucytuan/anaconda3/envs/IDS/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "t0, t1, t2 = np.split(merge.sample(frac=1, random_state=42), [int(.8*ds_size), int(.9*ds_size)])\n",
    "dataset = {x: Stock(s) for x, s in [(\"train\", t0), (\"valid\", t1), (\"test\", t2)]}\n",
    "loader = {x: DataLoader(dataset[x], batch_size=BZ, num_workers=24, shuffle=True) \n",
    "                                            for x in [\"train\", \"valid\", \"test\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/lucytuan/anaconda3/envs/IDS/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/lucytuan/anaconda3/envs/IDS/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Stock' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     sz \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m num, t \u001b[39min\u001b[39;00m loader[state]:\n\u001b[1;32m      4\u001b[0m         sz \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(num)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(sz)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/IDS/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for state in [\"train\", \"valid\", \"test\"]:\n",
    "    sz = 0\n",
    "    for num, t in loader[state]:\n",
    "        sz += len(num)\n",
    "    print(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert4price(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, text, mask):\n",
    "        _, output = self.bert(input_ids=text, attention_mask=mask,return_dict=False)\n",
    "        output = self.fc(output)\n",
    "        return torch.squeeze(output, 1).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = Bert4price()\n",
    "bert = bert.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crierion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(bert.parameters(), lr = LR, weight_decay=WD)\n",
    "lr_sch = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100/100]: 100%|██████████| 20/20 [00:02<00:00,  8.13it/s]\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(comment=COMMENT)\n",
    "min_loss = 1e10\n",
    "for epoch in range(EPS):\n",
    "    for state in [\"train\", \"valid\"]:\n",
    "        clear_output(wait=True)\n",
    "        tqdm_bar = tqdm(loader[state])\n",
    "        tqdm_bar.set_description(f\"[{epoch+1}/{EPS}]\")\n",
    "        loss_list = []\n",
    "        for value, content in tqdm_bar:\n",
    "            \n",
    "            text, mask = content[\"input_ids\"].squeeze(1), content[\"attention_mask\"]\n",
    "            text, mask = text.to(DEVICE), mask.to(DEVICE)\n",
    "        \n",
    "            value = value.to(DEVICE)\n",
    "            output = bert(text, mask)\n",
    "            loss = crierion(output, value)\n",
    "            loss_list.append(loss.item())\n",
    "            if state == \"train\":\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        avg_loss = sum(loss_list) / (len(loss_list)*BZ)\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(bert.state_dict(), f\"./pretrained/{COMMENT}.pt\")\n",
    "        writer.add_scalar(f\"{state}-loss\", avg_loss, epoch)\n",
    "    else:\n",
    "        lr_sch.step()\n",
    "        vnum = value.detach().cpu().numpy()\n",
    "        onum = output.detach().cpu().numpy()\n",
    "        result = np.array([vnum, onum]).T\n",
    "        valid_result = pd.DataFrame(result, columns = ['ans','output'])\n",
    "        valid_result.to_csv(\"../data/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('IDS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c5319918c1df56bbc8425fc11a4113500c53384b36f54418b1e7926749cda2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
